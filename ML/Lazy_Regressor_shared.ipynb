{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7969967,"sourceType":"datasetVersion","datasetId":4689503}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install lazypredict\n# Import necessary libraries\nfrom lazypredict.Supervised import LazyRegressor\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport numpy as np\nimport pandas as pd\n\n# Load the dataset\ndata = pd.read_excel(\"/kaggle/input/cars-occasion/unified_car_data.xlsx\", engine='openpyxl')\ndata = data.drop(columns=[\"État\", \"Année\",\"Description\",\"Nature\",\"Date annonce\"])\n# Drop rows with missing values\ndata.dropna(inplace=True)\n\n\ndata['Kilométrage'] = data['Kilométrage'].str.replace('km', '').str.replace(' ', '').str.strip()\ndata['Kilométrage'] = pd.to_numeric(data['Kilométrage'])\n\n\n\ndata['Puissance Fiscale'] = data['Puissance Fiscale'].str.replace('cv', '').str.strip()\ndata['Puissance Fiscale'] =  pd.to_numeric(data['Puissance Fiscale'])\n\n\ndata['Cylindrée'] = data['Cylindrée'].str.replace('cm³', '').str.replace(' ', '').str.strip()\ndata['Cylindrée'] =  pd.to_numeric(data['Cylindrée'])\n\n\n\ndata['Prix'] = data['Prix'].str.replace('DT', '').str.replace(' ', '').str.strip()\ndata['Prix'] =  pd.to_numeric(data['Prix'], errors='coerce')\ndata = data[pd.notna(data['Prix'])]\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ncategorical_cols = ['Marque', 'Modèle', 'Carrosserie','Transmission', 'Carburant', 'Boite Vitesse', 'Couleur exterieure','Sellerie', 'Couleur interieure']\nfor col in categorical_cols:\n    data[col] = label_encoder.fit_transform(data[col])\n\n# Convert 'Mise en circulation' to datetime and extract year\ndata['Mise en circulation'] = pd.to_datetime(data['Mise en circulation'])\ndata['Year'] = data['Mise en circulation'].dt.year\n\n# Drop 'Mise en circulation' column after extracting year\ndata.drop(columns=['Mise en circulation'], inplace=True)\n\n# Shuffle the dataset\ndata_shuffled = shuffle(data, random_state=13)\n\n# Separate features and target variable\nX = data_shuffled.drop(columns=[\"Prix\"])  # Adjust target_column_name\ny = data_shuffled[\"Prix\"]  # Adjust target_column_name\n\n# Convert X to float32\nX = X.astype(np.float32)\n\n# Split the dataset into train and test sets\noffset = int(X.shape[0] * 0.9)\nX_train, y_train = X[:offset], y[:offset]\nX_test, y_test = X[offset:], y[offset:]\n\n# Initialize LazyRegressor\nreg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n\n# Fit LazyRegressor\nmodels, predictions = reg.fit(X_train, X_test, y_train, y_test)\n\n# Print the models and their performance\nprint(models)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-18T11:28:39.187547Z","iopub.execute_input":"2024-04-18T11:28:39.187937Z","iopub.status.idle":"2024-04-18T11:29:11.201844Z","shell.execute_reply.started":"2024-04-18T11:28:39.187909Z","shell.execute_reply":"2024-04-18T11:29:11.200840Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: lazypredict in /opt/conda/lib/python3.10/site-packages (0.2.12)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from lazypredict) (8.1.7)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from lazypredict) (1.2.2)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from lazypredict) (2.2.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from lazypredict) (4.66.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from lazypredict) (1.3.2)\nRequirement already satisfied: lightgbm in /opt/conda/lib/python3.10/site-packages (from lazypredict) (4.2.0)\nRequirement already satisfied: xgboost in /opt/conda/lib/python3.10/site-packages (from lazypredict) (2.0.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from lightgbm->lazypredict) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm->lazypredict) (1.11.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->lazypredict) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->lazypredict) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->lazypredict) (2023.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->lazypredict) (3.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->lazypredict) (1.16.0)\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▌  | 32/42 [00:12<00:03,  2.69it/s]","output_type":"stream"},{"name":"stdout","text":"QuantileRegressor model failed to execute\nSolver interior-point is not anymore available in SciPy >= 1.11.0.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 42/42 [00:15<00:00,  2.76it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002049 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 731\n[LightGBM] [Info] Number of data points in the train set: 2194, number of used features: 14\n[LightGBM] [Info] Start training from score 87154.056518\n                               Adjusted R-Squared  R-Squared      RMSE  \\\nModel                                                                    \nXGBRegressor                                 0.98       0.98  10427.75   \nExtraTreesRegressor                          0.98       0.98  10595.58   \nGaussianProcessRegressor                     0.95       0.95  15552.16   \nRandomForestRegressor                        0.94       0.95  16793.14   \nBaggingRegressor                             0.93       0.93  19070.62   \nDecisionTreeRegressor                        0.90       0.91  21826.35   \nExtraTreeRegressor                           0.90       0.91  22237.81   \nGradientBoostingRegressor                    0.90       0.90  22341.24   \nHistGradientBoostingRegressor                0.90       0.90  22585.43   \nLGBMRegressor                                0.89       0.90  23013.61   \nKNeighborsRegressor                          0.61       0.63  43936.73   \nAdaBoostRegressor                            0.60       0.62  44447.36   \nPoissonRegressor                             0.50       0.53  49466.33   \nHuberRegressor                               0.39       0.43  54585.86   \nSGDRegressor                                 0.39       0.43  54770.92   \nLassoLarsCV                                  0.39       0.43  54800.57   \nLarsCV                                       0.39       0.43  54800.57   \nLassoCV                                      0.39       0.42  54804.12   \nLassoLarsIC                                  0.39       0.42  54895.74   \nPassiveAggressiveRegressor                   0.38       0.42  54924.46   \nBayesianRidge                                0.38       0.42  54930.52   \nRidgeCV                                      0.38       0.42  54942.38   \nRidge                                        0.38       0.42  54961.62   \nLasso                                        0.38       0.42  54963.42   \nLassoLars                                    0.38       0.42  54963.43   \nLinearRegression                             0.38       0.42  54963.82   \nTransformedTargetRegressor                   0.38       0.42  54963.82   \nLars                                         0.38       0.42  54963.82   \nElasticNet                                   0.38       0.42  55014.56   \nOrthogonalMatchingPursuitCV                  0.38       0.42  55037.74   \nGammaRegressor                               0.38       0.42  55192.78   \nTweedieRegressor                             0.36       0.40  56103.55   \nOrthogonalMatchingPursuit                    0.18       0.23  63511.57   \nRANSACRegressor                              0.03       0.09  68887.94   \nElasticNetCV                                -0.02       0.04  70639.22   \nDummyRegressor                              -0.07      -0.00  72322.71   \nNuSVR                                       -0.08      -0.02  72839.04   \nSVR                                         -0.14      -0.07  74806.56   \nKernelRidge                                 -0.96      -0.84  98012.06   \nMLPRegressor                                -1.40      -1.25 108378.02   \nLinearSVR                                   -1.45      -1.29 109475.04   \n\n                               Time Taken  \nModel                                      \nXGBRegressor                         0.22  \nExtraTreesRegressor                  0.64  \nGaussianProcessRegressor             1.25  \nRandomForestRegressor                1.02  \nBaggingRegressor                     0.11  \nDecisionTreeRegressor                0.11  \nExtraTreeRegressor                   0.07  \nGradientBoostingRegressor            0.35  \nHistGradientBoostingRegressor        0.41  \nLGBMRegressor                        0.15  \nKNeighborsRegressor                  0.12  \nAdaBoostRegressor                    0.20  \nPoissonRegressor                     0.06  \nHuberRegressor                       0.10  \nSGDRegressor                         0.09  \nLassoLarsCV                          0.13  \nLarsCV                               0.13  \nLassoCV                              0.37  \nLassoLarsIC                          0.11  \nPassiveAggressiveRegressor           0.15  \nBayesianRidge                        0.09  \nRidgeCV                              0.12  \nRidge                                0.05  \nLasso                                0.09  \nLassoLars                            0.09  \nLinearRegression                     0.09  \nTransformedTargetRegressor           0.05  \nLars                                 0.06  \nElasticNet                           0.05  \nOrthogonalMatchingPursuitCV          0.09  \nGammaRegressor                       0.06  \nTweedieRegressor                     0.12  \nOrthogonalMatchingPursuit            0.05  \nRANSACRegressor                      0.74  \nElasticNetCV                         0.36  \nDummyRegressor                       0.01  \nNuSVR                                0.32  \nSVR                                  0.33  \nKernelRidge                          0.54  \nMLPRegressor                         5.86  \nLinearSVR                            0.04  \n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}